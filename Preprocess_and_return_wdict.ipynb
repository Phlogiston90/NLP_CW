{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 103/13240 [00:00<00:12, 1026.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                              tweet subtask_a  \\\n",
      "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
      "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
      "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
      "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
      "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
      "\n",
      "  subtask_b subtask_c  \n",
      "0       UNT      NULL  \n",
      "1       TIN       IND  \n",
      "2      NULL      NULL  \n",
      "3       UNT      NULL  \n",
      "4      NULL      NULL  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13240/13240 [00:03<00:00, 3990.88it/s]\n",
      "100%|██████████| 1193515/1193515 [00:18<00:00, 64071.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm \n",
    "import codecs\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import zipfile\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# we fix the seeds to get consistent results\n",
    "\n",
    "SEED = 234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "# the following makes it determnisitic but may have performance impact:\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Load data - from Github\n",
    "train_url = 'https://raw.githubusercontent.com/JeanKaddour/OffensEval2019-TeamMDJ/master/data/start_kit/training-v1/offenseval-training-v1.tsv'\n",
    "val_url   = 'https://raw.githubusercontent.com/JeanKaddour/OffensEval2019-TeamMDJ/master/data/start_kit/trial-data/offenseval-trial.txt'\n",
    "test1_url = 'https://raw.githubusercontent.com/JeanKaddour/OffensEval2019-TeamMDJ/master/data/test_a_release/testset-taska.tsv'\n",
    "test2_url = 'https://raw.githubusercontent.com/JeanKaddour/OffensEval2019-TeamMDJ/master/data/test_b_release/testset-taskb.tsv'\n",
    "test3_url = 'https://raw.githubusercontent.com/JeanKaddour/OffensEval2019-TeamMDJ/master/data/test_c_release/test_set_taskc.tsv'\n",
    "train_set = pd.read_csv(train_url, sep='\\t', na_filter=False)\n",
    "val_set   = pd.read_csv(val_url, sep='\\t', na_filter=False)\n",
    "test_set1 = pd.read_csv(test1_url, sep='\\t', na_filter=False)\n",
    "test_set2 = pd.read_csv(test2_url, sep='\\t', na_filter=False)\n",
    "test_set3 = pd.read_csv(test3_url, sep='\\t', na_filter=False)\n",
    "\n",
    "#wvecs = np.load(\"wvecs.npy\")\n",
    "print(train_set.head())\n",
    "\n",
    "\"\"\"Tokenizes and preprocesses a corpus. Adapted from Romain Paulus and Jeffrey\\\n",
    "Pennington from https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb but\n",
    "changed it to make it work\"\"\"\n",
    "def get_tokenized_corpus(corpus, verbose=False):\n",
    "  tokenized_corpus = []\n",
    "  for sentence in tqdm(corpus):\n",
    "    tokenized_sentence = []\n",
    "    for token in sentence.split(' '):\n",
    "      token = token.strip()\n",
    "      if token == '':\n",
    "        continue\n",
    "      # Detect URLs\n",
    "      match = re.search(\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\",token)\n",
    "      if match:\n",
    "        token = '<url> ' + match[0]\n",
    "        if verbose: print(\"url: \", token)\n",
    "      token = token.replace(\"@USER\", \"<user>\")\n",
    "      token = re.sub(\"@\\w+\",\"<user>\",token)\n",
    "      token = re.sub(\"[8:=;]['`\\-][)d]+|[)d]+['`\\-][8:=;]\",\" <smile>\",token)\n",
    "      token = re.sub(\"[8:=;]['`\\-]p+\",\" <lolface>\",token)\n",
    "      token = re.sub(\"[8:=;]['`\\-]\\(+|\\)+['`\\-][8:=;]\",\" <sadface>\",token)\n",
    "      token = re.sub(\"[8:=;]['`\\-][\\/|l*]\", \" <neutralface>\",token)\n",
    "      token = re.sub(\"<3\", \" <heart>\", token)\n",
    "      token = re.sub(\"^[-+]?[.\\d]*[\\d]+[:,.\\d]*$\", \" <number>\", token)\n",
    "      # Split hashtags on uppercase letters\n",
    "      hashtag = re.search(\"#\\S+\", token)\n",
    "      if hashtag:\n",
    "        hashtag_body = hashtag[0][1:]\n",
    "        if hashtag_body == hashtag_body.upper():\n",
    "          token = \"<hashtag> \" + hashtag_body + \" <hashtag_end>\"\n",
    "        else:\n",
    "          token = \"<hashtag> \"\n",
    "          parts = re.findall(\"[A-Z]*[a-z]+\", hashtag_body)\n",
    "          for part in parts:\n",
    "            token += part + \" \"\n",
    "          token += \"<hashtag_end>\"\n",
    "        if verbose: print(\"hashtag: \", token)\n",
    "      # Mark punctuation repetitions (eg. \"!!!\" => \"! <REPEAT>\"\n",
    "      match = re.search(\"(?P<word>[^!.,?:;]*)(?P<punctuation>[!.,?:;]){2,}$\", token)\n",
    "      if match:\n",
    "        token = match[\"word\"] + \" \" + match[\"punctuation\"] + \" <repeat>\"\n",
    "        if verbose: print(\"rep punct: \", token)\n",
    "      # Split single punctuation:\n",
    "      else:\n",
    "        match = re.search(\"(?P<word>.*)(?P<punctuation>[!.,?:;])$\", token)\n",
    "        if match:\n",
    "          token = match[\"word\"] + \" \" + match[\"punctuation\"]\n",
    "          if verbose: print(\"single punct: \", token)\n",
    "      match = re.search(\"(?P<p>.* *)(?P<w>[A-Z]{2,})(?P<r>.*)\",token)\n",
    "      if match:\n",
    "        if match[\"r\"] != \" \":\n",
    "          token = match[\"p\"] + match[\"w\"] + \" <allcaps> \" + match[\"r\"]\n",
    "        else:\n",
    "          token = match[\"p\"] + match[\"w\"] + \" <allcaps>\"\n",
    "        if verbose: print(\"allcaps: \", token)\n",
    "      token = token.lower()\n",
    "      token = token.strip()\n",
    "      tokens = token.split(\" \")\n",
    "      for token in tokens:\n",
    "        if token != \"\":\n",
    "          tokenized_sentence.append(token)\n",
    "    tokenized_corpus.append(tokenized_sentence)\n",
    "  return tokenized_corpus\n",
    "\n",
    "def print_corpus_tokens(tokenized_corpus, corpus, sentences):\n",
    "  for sent in sentences:\n",
    "    print(corpus.values[sent,1])\n",
    "    print(tokenized_corpus[sent])\n",
    "    print()\n",
    "    \n",
    "def get_word2idx(tokenized_corpus):\n",
    "  vocabulary = []\n",
    "  for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
    "  # we reserve the 0 index for the placeholder token\n",
    "  word2idx['<pad>'] = 0\n",
    "  return word2idx\n",
    "\n",
    "tokenized_train_corpus = get_tokenized_corpus(train_set.values[:,1])\n",
    "word2idx = get_word2idx(tokenized_train_corpus)\n",
    "\n",
    "wvecs = np.zeros((len(word2idx), 100))\n",
    "\n",
    "with codecs.open('glove.twitter.27B/glove.twitter.27B.100d.txt', 'r','utf-8') as f: \n",
    "  index = 0\n",
    "  for line in tqdm(f.readlines()):\n",
    "    if len(line.strip().split()) > 3:\n",
    "      word = line.strip().split()[0]\n",
    "      if word in word2idx:\n",
    "          (word, vec) = (word, list(map(float,line.strip().split()[1:])))\n",
    "          idx = word2idx[word]\n",
    "          wvecs[idx] = vec\n",
    "      np.random.normal(scale=0.01, size=(100, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"wvecs.npy\",wvecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
